{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5af4ef51-b28a-42a9-afb7-f36cd2c58a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 759ms/step - accuracy: 0.1473 - loss: 4.1590\n",
      "Epoch 2/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 769ms/step - accuracy: 0.3120 - loss: 3.6697\n",
      "Epoch 3/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 706ms/step - accuracy: 0.3274 - loss: 3.2303\n",
      "Epoch 4/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 714ms/step - accuracy: 0.3291 - loss: 2.9853\n",
      "Epoch 5/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 728ms/step - accuracy: 0.3267 - loss: 2.8844\n",
      "Training History: {'accuracy': [0.23788614571094513, 0.3361797332763672, 0.33408981561660767, 0.3351128399372101, 0.337586373090744], 'loss': [4.084088325500488, 3.542590618133545, 3.1490907669067383, 2.940845489501953, 2.8420093059539795]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "data = np.load(r\"D:\\CODSOFT-TASK\\TASK5-Handwritten-text-Generator\\deepwriting_validation.npz\", allow_pickle=True)\n",
    "X = data['strokes']\n",
    "y = data['char_labels']\n",
    "\n",
    "# Flatten the y array and find unique elements\n",
    "flat_y = np.concatenate(y)\n",
    "num_classes = np.max(flat_y) + 1  \n",
    "\n",
    "# Calculate mean and std for normalization\n",
    "mean = np.mean([np.mean(seq, axis=0) for seq in X], axis=0)\n",
    "std = np.std([np.std(seq, axis=0) for seq in X], axis=0)\n",
    "epsilon = 1e-8\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_padded = pad_sequences(X_batch, padding='post', dtype='float32', maxlen=None)\n",
    "    y_padded = pad_sequences(y_batch, padding='post')\n",
    "    X_normalized = (X_padded - mean) / (std + epsilon)\n",
    "    y_categorical = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_padded])\n",
    "    return X_normalized, y_categorical\n",
    "\n",
    "def generator(X, y, batch_size=64):\n",
    "    while True:\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            end = min(start + batch_size, len(X))\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            yield preprocess(X_batch, y_batch)\n",
    "\n",
    "# Determine the shape of the input data after padding\n",
    "X_padded, _ = preprocess(X[:1], y[:1])\n",
    "input_shape = (X_padded.shape[1], X_padded.shape[2])\n",
    "\n",
    "# Define the model\n",
    "# model = Sequential([\n",
    "#     LSTM(32, return_sequences=True, input_shape=input_shape),  # Increased LSTM units for better capacity\n",
    "#     TimeDistributed(Dense(num_classes, activation='softmax'))\n",
    "# ])\n",
    "model = Sequential([\n",
    "  LSTM(64, return_sequences=True, input_shape=input_shape),  # Increased LSTM units\n",
    "  LSTM(32, return_sequences=True),  # Added another LSTM layer\n",
    "  TimeDistributed(Dense(num_classes, activation='softmax'))\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(X) // batch_size\n",
    "history = model.fit(generator(X, y, batch_size=batch_size), steps_per_epoch=steps_per_epoch, epochs=5, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model for future use\n",
    "from tensorflow.keras.saving import save_model\n",
    "model.save('handwriting_model.keras')\n",
    "# Debugging: Check training history\n",
    "print(\"Training History:\", history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b05cf3-dbd6-458a-85c0-6cb613d2a497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
